#!/usr/bin/env python3
"""
é«˜åº¦ãªçµ±è¨ˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³
å› æœæ¨è«–ãƒ»é‡å›å¸°åˆ†æãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã«ã‚ˆã‚‹è¨˜äº‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç§‘å­¦çš„åˆ†æ
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# çµ±è¨ˆãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.metrics import silhouette_score, r2_score
    from sklearn.model_selection import train_test_split
    from scipy import stats
    from scipy.stats import ttest_ind, chi2_contingency
    import statsmodels.api as sm
    from statsmodels.tsa.seasonal import seasonal_decompose
    from statsmodels.stats.outliers_influence import variance_inflation_factor
except ImportError:
    print("è­¦å‘Š: çµ±è¨ˆåˆ†æãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚pip install scikit-learn scipy statsmodels ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")


@dataclass
class ArticlePerformanceData:
    """è¨˜äº‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿"""
    article_id: str
    title: str
    publish_date: datetime
    word_count: int
    keyword_density: float
    seo_score: float
    tone_manner: str
    author: str
    category: str
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
    pv_daily: List[int]
    unique_users: List[int] 
    avg_time_on_page: List[float]
    bounce_rate: List[float]
    social_shares: List[int]
    conversions: List[int]
    
    # SEOæŒ‡æ¨™
    search_impressions: List[int]
    search_clicks: List[int]
    avg_position: List[float]
    
    # ã‚¿ã‚°ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    tags: List[str]
    promotion_activities: List[str]
    external_events: List[str]


@dataclass 
class CausalInferenceResult:
    """å› æœæ¨è«–çµæœ"""
    method: str
    treatment_effect: float
    confidence_interval: Tuple[float, float]
    p_value: float
    statistical_significance: bool
    effect_size: str
    interpretation: str
    recommendations: List[str]


class AdvancedStatisticalAnalyzer:
    """é«˜åº¦çµ±è¨ˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoders = {}
        
    # ============================================================
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ»æº–å‚™
    # ============================================================
    
    def prepare_analysis_dataset(self, articles_data: List[ArticlePerformanceData]) -> pd.DataFrame:
        """åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™"""
        print("ğŸ“Š åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­...")
        
        records = []
        for article in articles_data:
            # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹
            for i, (pv, users, time_on_page, bounce_rate, shares, conversions) in enumerate(
                zip(article.pv_daily, article.unique_users, article.avg_time_on_page,
                    article.bounce_rate, article.social_shares, article.conversions)
            ):
                record = {
                    # è¨˜äº‹å±æ€§
                    'article_id': article.article_id,
                    'title': article.title,
                    'publish_date': article.publish_date,
                    'day_since_publish': i,
                    'word_count': article.word_count,
                    'keyword_density': article.keyword_density,
                    'seo_score': article.seo_score,
                    'tone_manner': article.tone_manner,
                    'author': article.author,
                    'category': article.category,
                    
                    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
                    'daily_pv': pv,
                    'daily_users': users,
                    'avg_time_on_page': time_on_page,
                    'bounce_rate': bounce_rate,
                    'social_shares': shares,
                    'conversions': conversions,
                    
                    # SEOæŒ‡æ¨™ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                    'search_impressions': article.search_impressions[i] if i < len(article.search_impressions) else 0,
                    'search_clicks': article.search_clicks[i] if i < len(article.search_clicks) else 0,
                    'avg_position': article.avg_position[i] if i < len(article.avg_position) else 0,
                    
                    # è¨ˆç®—æŒ‡æ¨™
                    'ctr': (article.search_clicks[i] / article.search_impressions[i]) if i < len(article.search_impressions) and article.search_impressions[i] > 0 else 0,
                    'conversion_rate': conversions / users if users > 0 else 0,
                    'engagement_score': (time_on_page * (1 - bounce_rate) * shares) if bounce_rate < 1 else 0,
                    
                    # æ™‚é–“å¤‰æ•°
                    'weekday': (article.publish_date + timedelta(days=i)).weekday(),
                    'month': (article.publish_date + timedelta(days=i)).month,
                    'is_weekend': (article.publish_date + timedelta(days=i)).weekday() >= 5,
                    
                    # ã‚¿ã‚°ãƒ»ãƒ•ãƒ©ã‚°ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
                    'has_images': 'images' in article.tags,
                    'has_video': 'video' in article.tags,
                    'promoted_on_social': any('social' in activity for activity in article.promotion_activities),
                    'email_promoted': any('email' in activity for activity in article.promotion_activities),
                }
                records.append(record)
        
        df = pd.DataFrame(records)
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†: {len(df):,}è¡Œ, {len(df.columns)}åˆ—")
        return df
    
    def encode_categorical_variables(self, df: pd.DataFrame) -> pd.DataFrame:
        """ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰"""
        categorical_columns = ['tone_manner', 'author', 'category']
        
        for col in categorical_columns:
            if col in df.columns:
                if col not in self.label_encoders:
                    self.label_encoders[col] = LabelEncoder()
                    df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(df[col].astype(str))
                else:
                    df[f'{col}_encoded'] = self.label_encoders[col].transform(df[col].astype(str))
        
        return df
    
    # ============================================================
    # 1. å·®åˆ†ã®å·®åˆ†æ³• (Difference-in-Differences)
    # ============================================================
    
    def difference_in_differences_analysis(
        self, 
        df: pd.DataFrame,
        treatment_articles: List[str],
        control_articles: List[str],
        intervention_date: datetime,
        outcome_variable: str = 'daily_pv'
    ) -> CausalInferenceResult:
        """
        å·®åˆ†ã®å·®åˆ†æ³•ã«ã‚ˆã‚‹å› æœåŠ¹æœæ¨å®š
        
        Args:
            df: åˆ†æãƒ‡ãƒ¼ã‚¿
            treatment_articles: å‡¦ç½®ç¾¤è¨˜äº‹ID
            control_articles: å¯¾ç…§ç¾¤è¨˜äº‹ID  
            intervention_date: ä»‹å…¥æ—¥
            outcome_variable: ã‚¢ã‚¦ãƒˆã‚«ãƒ å¤‰æ•°
        """
        print(f"ğŸ“ˆ å·®åˆ†ã®å·®åˆ†æ³•åˆ†æé–‹å§‹: {outcome_variable}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        analysis_df = df[
            (df['article_id'].isin(treatment_articles + control_articles))
        ].copy()
        
        # å‡¦ç½®ãƒ»æ™‚æœŸãƒ€ãƒŸãƒ¼å¤‰æ•°ä½œæˆ
        analysis_df['treated'] = analysis_df['article_id'].isin(treatment_articles).astype(int)
        analysis_df['post_intervention'] = (analysis_df['publish_date'] >= intervention_date).astype(int)
        analysis_df['did_interaction'] = analysis_df['treated'] * analysis_df['post_intervention']
        
        # DIDå›å¸°ãƒ¢ãƒ‡ãƒ«
        X = analysis_df[['treated', 'post_intervention', 'did_interaction', 'day_since_publish', 'weekday']]
        X = sm.add_constant(X)
        y = analysis_df[outcome_variable]
        
        model = sm.OLS(y, X).fit()
        
        # çµæœã®è§£é‡ˆ
        did_coefficient = model.params['did_interaction']
        p_value = model.pvalues['did_interaction']
        conf_int = model.conf_int().loc['did_interaction']
        
        # åŠ¹æœã‚µã‚¤ã‚ºã®åˆ¤å®š
        baseline_mean = analysis_df[
            (analysis_df['treated'] == 1) & (analysis_df['post_intervention'] == 0)
        ][outcome_variable].mean()
        
        effect_size_pct = (did_coefficient / baseline_mean) * 100 if baseline_mean > 0 else 0
        
        if abs(effect_size_pct) >= 20:
            effect_size = "å¤§"
        elif abs(effect_size_pct) >= 10:
            effect_size = "ä¸­"
        else:
            effect_size = "å°"
        
        # çµæœç”Ÿæˆ
        result = CausalInferenceResult(
            method="å·®åˆ†ã®å·®åˆ†æ³• (DID)",
            treatment_effect=did_coefficient,
            confidence_interval=(conf_int[0], conf_int[1]),
            p_value=p_value,
            statistical_significance=p_value < 0.05,
            effect_size=effect_size,
            interpretation=f"ä»‹å…¥ã«ã‚ˆã‚Š{outcome_variable}ãŒ{did_coefficient:.2f}å˜ä½ï¼ˆ{effect_size_pct:.1f}%ï¼‰å¤‰åŒ–",
            recommendations=self._generate_did_recommendations(did_coefficient, p_value, effect_size_pct)
        )
        
        print(f"âœ… DIDåˆ†æå®Œäº†: åŠ¹æœ={did_coefficient:.2f}, på€¤={p_value:.4f}")
        return result
    
    def _generate_did_recommendations(self, effect: float, p_value: float, effect_size_pct: float) -> List[str]:
        """DIDåˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if p_value < 0.05:
            if effect > 0:
                recommendations.append(f"ä»‹å…¥ã«ã‚ˆã‚Š{effect_size_pct:.1f}%ã®æœ‰æ„ãªæ”¹å–„ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã“ã®æ–½ç­–ã‚’ä»–ã®è¨˜äº‹ã«ã‚‚é©ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
                if effect_size_pct >= 20:
                    recommendations.append("åŠ¹æœãŒå¤§ãã„ãŸã‚ã€é¡ä¼¼è¨˜äº‹ã¸ã®å±•é–‹ã‚’å„ªå…ˆçš„ã«æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            else:
                recommendations.append(f"ä»‹å…¥ã«ã‚ˆã‚Š{abs(effect_size_pct):.1f}%ã®æœ‰æ„ãªæ‚ªåŒ–ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã“ã®æ–½ç­–ã¯é¿ã‘ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        else:
            recommendations.append("çµ±è¨ˆçš„ã«æœ‰æ„ãªåŠ¹æœã¯ç¢ºèªã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®æ‹¡å¤§ã¾ãŸã¯ä»–ã®è¦å› ã®æ¤œè¨ãŒå¿…è¦ã§ã™ã€‚")
        
        return recommendations
    
    # ============================================================
    # 2. é‡å›å¸°åˆ†æ
    # ============================================================
    
    def multiple_regression_analysis(
        self, 
        df: pd.DataFrame,
        target_variable: str = 'daily_pv',
        feature_columns: List[str] = None
    ) -> Dict[str, Any]:
        """
        é‡å›å¸°åˆ†æã«ã‚ˆã‚‹è¦å› åˆ†æ
        
        Args:
            df: åˆ†æãƒ‡ãƒ¼ã‚¿
            target_variable: ç›®çš„å¤‰æ•°
            feature_columns: èª¬æ˜å¤‰æ•°ãƒªã‚¹ãƒˆ
        """
        print(f"ğŸ“Š é‡å›å¸°åˆ†æé–‹å§‹: ç›®çš„å¤‰æ•°={target_variable}")
        
        if feature_columns is None:
            feature_columns = [
                'word_count', 'keyword_density', 'seo_score',
                'tone_manner_encoded', 'author_encoded', 'category_encoded',
                'day_since_publish', 'weekday', 'has_images', 'has_video',
                'promoted_on_social', 'email_promoted'
            ]
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        analysis_df = df.dropna(subset=[target_variable] + feature_columns).copy()
        
        X = analysis_df[feature_columns]
        y = analysis_df[target_variable]
        
        # å¤šé‡å…±ç·šæ€§ãƒã‚§ãƒƒã‚¯
        vif_data = pd.DataFrame()
        vif_data["Feature"] = X.columns
        vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        model = LinearRegression()
        model.fit(X_train, y_train)
        
        # äºˆæ¸¬ãƒ»è©•ä¾¡
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        
        r2_train = r2_score(y_train, y_pred_train)
        r2_test = r2_score(y_test, y_pred_test)
        
        # çµ±è¨ˆçš„æ¤œå®šï¼ˆstatsmodelsï¼‰
        X_sm = sm.add_constant(X)
        model_sm = sm.OLS(y, X_sm).fit()
        
        # ä¿‚æ•°ã®è§£é‡ˆ
        coefficients = pd.DataFrame({
            'feature': X.columns,
            'coefficient': model.coef_,
            'p_value': model_sm.pvalues[1:],  # å®šæ•°é …ã‚’é™¤ã
            'significant': model_sm.pvalues[1:] < 0.05
        }).sort_values('coefficient', key=abs, ascending=False)
        
        # æ´å¯Ÿç”Ÿæˆ
        insights = self._generate_regression_insights(coefficients, target_variable)
        
        result = {
            'model_performance': {
                'r2_train': r2_train,
                'r2_test': r2_test,
                'adjusted_r2': model_sm.rsquared_adj,
                'f_statistic': model_sm.fvalue,
                'f_p_value': model_sm.f_pvalue
            },
            'coefficients': coefficients.to_dict('records'),
            'multicollinearity': vif_data.to_dict('records'),
            'feature_importance': dict(zip(X.columns, abs(model.coef_))),
            'insights': insights,
            'model_equation': self._generate_model_equation(coefficients),
            'recommendations': self._generate_regression_recommendations(coefficients, r2_test)
        }
        
        print(f"âœ… é‡å›å¸°åˆ†æå®Œäº†: RÂ²={r2_test:.3f}")
        return result
    
    def _generate_regression_insights(self, coefficients: pd.DataFrame, target_variable: str) -> List[str]:
        """å›å¸°åˆ†æçµæœã®æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        # æœ€ã‚‚å½±éŸ¿ã®å¤§ãã„æ­£ã®è¦å› 
        positive_factors = coefficients[
            (coefficients['coefficient'] > 0) & (coefficients['significant'])
        ].head(3)
        
        if not positive_factors.empty:
            top_positive = positive_factors.iloc[0]
            insights.append(
                f"{top_positive['feature']}ãŒ{target_variable}ã«æœ€ã‚‚å¤§ããªæ­£ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™"
                f"ï¼ˆä¿‚æ•°: {top_positive['coefficient']:.2f}ï¼‰"
            )
        
        # æœ€ã‚‚å½±éŸ¿ã®å¤§ãã„è² ã®è¦å› 
        negative_factors = coefficients[
            (coefficients['coefficient'] < 0) & (coefficients['significant'])
        ].head(3)
        
        if not negative_factors.empty:
            top_negative = negative_factors.iloc[0]
            insights.append(
                f"{top_negative['feature']}ãŒ{target_variable}ã«æœ€ã‚‚å¤§ããªè² ã®å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™"
                f"ï¼ˆä¿‚æ•°: {top_negative['coefficient']:.2f}ï¼‰"
            )
        
        # æœ‰æ„ã§ãªã„è¦å› 
        non_significant = coefficients[~coefficients['significant']]
        if len(non_significant) > 0:
            insights.append(f"{len(non_significant)}å€‹ã®è¦å› ã¯çµ±è¨ˆçš„ã«æœ‰æ„ãªå½±éŸ¿ã‚’ç¤ºã—ã¦ã„ã¾ã›ã‚“")
        
        return insights
    
    def _generate_model_equation(self, coefficients: pd.DataFrame) -> str:
        """ãƒ¢ãƒ‡ãƒ«æ–¹ç¨‹å¼ã®ç”Ÿæˆ"""
        terms = []
        for _, row in coefficients.iterrows():
            coef = row['coefficient']
            feature = row['feature']
            if coef >= 0:
                terms.append(f"+{coef:.3f}*{feature}")
            else:
                terms.append(f"{coef:.3f}*{feature}")
        
        equation = "äºˆæ¸¬å€¤ = å®šæ•°é … " + " ".join(terms)
        return equation
    
    def _generate_regression_recommendations(self, coefficients: pd.DataFrame, r2: float) -> List[str]:
        """å›å¸°åˆ†æã«åŸºã¥ãæ¨å¥¨äº‹é …"""
        recommendations = []
        
        if r2 >= 0.7:
            recommendations.append("ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜åŠ›ãŒé«˜ãã€è¦å› åˆ†æã®ä¿¡é ¼æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        elif r2 >= 0.5:
            recommendations.append("ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜åŠ›ã¯ä¸­ç¨‹åº¦ã§ã™ã€‚è¿½åŠ ã®èª¬æ˜å¤‰æ•°ã®æ¤œè¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        else:
            recommendations.append("ãƒ¢ãƒ‡ãƒ«ã®èª¬æ˜åŠ›ãŒä½ã„ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®è³ªã‚„å¤‰æ•°é¸æŠã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚")
        
        # æœ‰æ„ãªæ­£ã®è¦å› ã«åŸºã¥ãæ¨å¥¨
        significant_positive = coefficients[
            (coefficients['coefficient'] > 0) & (coefficients['significant'])
        ]
        
        for _, factor in significant_positive.head(3).iterrows():
            recommendations.append(
                f"{factor['feature']}ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§{factor['coefficient']:.2f}ã®æ”¹å–„åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™"
            )
        
        return recommendations
    
    # ============================================================
    # 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
    # ============================================================
    
    def cluster_analysis(
        self, 
        df: pd.DataFrame,
        feature_columns: List[str] = None,
        n_clusters: int = None
    ) -> Dict[str, Any]:
        """
        ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã«ã‚ˆã‚‹è¨˜äº‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†é¡
        
        Args:
            df: åˆ†æãƒ‡ãƒ¼ã‚¿
            feature_columns: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨ç‰¹å¾´é‡
            n_clusters: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ±ºå®šï¼‰
        """
        print("ğŸ¯ ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æé–‹å§‹")
        
        if feature_columns is None:
            feature_columns = [
                'word_count', 'keyword_density', 'seo_score',
                'daily_pv', 'avg_time_on_page', 'bounce_rate',
                'social_shares', 'conversions', 'conversion_rate'
            ]
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        analysis_df = df.dropna(subset=feature_columns).copy()
        
        # ç‰¹å¾´é‡æ¨™æº–åŒ–
        X = self.scaler.fit_transform(analysis_df[feature_columns])
        
        # æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®æ±ºå®šï¼ˆã‚¨ãƒ«ãƒœãƒ¼æ³• + ã‚·ãƒ«ã‚¨ãƒƒãƒˆåˆ†æï¼‰
        if n_clusters is None:
            n_clusters = self._find_optimal_clusters(X)
        
        # K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ 
        analysis_df['cluster'] = cluster_labels
        
        # å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç‰¹å¾´åˆ†æ
        cluster_profiles = self._analyze_cluster_profiles(analysis_df, feature_columns)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å“è³ªè©•ä¾¡
        silhouette_avg = silhouette_score(X, cluster_labels)
        
        # DBSCANã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡ºã‚‚å®Ÿè¡Œ
        dbscan = DBSCAN(eps=0.5, min_samples=5)
        dbscan_labels = dbscan.fit_predict(X)
        outliers = np.sum(dbscan_labels == -1)
        
        result = {
            'n_clusters': n_clusters,
            'silhouette_score': silhouette_avg,
            'cluster_profiles': cluster_profiles,
            'outliers_detected': outliers,
            'cluster_assignments': dict(zip(analysis_df['article_id'], cluster_labels)),
            'cluster_insights': self._generate_cluster_insights(cluster_profiles),
            'recommendations': self._generate_cluster_recommendations(cluster_profiles)
        }
        
        print(f"âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æå®Œäº†: {n_clusters}ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼, ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°={silhouette_avg:.3f}")
        return result
    
    def _find_optimal_clusters(self, X: np.ndarray, max_clusters: int = 10) -> int:
        """æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã®æ±ºå®š"""
        inertias = []
        silhouette_scores = []
        
        for k in range(2, max_clusters + 1):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X)
            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(X, labels))
        
        # ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°ãŒæœ€å¤§ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ•°ã‚’é¸æŠ
        optimal_k = np.argmax(silhouette_scores) + 2
        return optimal_k
    
    def _analyze_cluster_profiles(self, df: pd.DataFrame, feature_columns: List[str]) -> Dict[int, Dict]:
        """å„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        cluster_profiles = {}
        
        for cluster_id in sorted(df['cluster'].unique()):
            cluster_data = df[df['cluster'] == cluster_id]
            
            profile = {
                'size': len(cluster_data),
                'percentage': len(cluster_data) / len(df) * 100,
                'feature_means': cluster_data[feature_columns].mean().to_dict(),
                'feature_stds': cluster_data[feature_columns].std().to_dict(),
                'top_articles': cluster_data.nlargest(5, 'daily_pv')['article_id'].tolist(),
                'common_characteristics': self._identify_cluster_characteristics(cluster_data, feature_columns)
            }
            
            cluster_profiles[cluster_id] = profile
        
        return cluster_profiles
    
    def _identify_cluster_characteristics(self, cluster_data: pd.DataFrame, feature_columns: List[str]) -> List[str]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç‰¹å¾´çš„ãªæ€§è³ªã‚’ç‰¹å®š"""
        characteristics = []
        
        # å…¨ä½“å¹³å‡ã¨æ¯”è¼ƒ
        overall_means = cluster_data[feature_columns].mean()
        
        # ç‰¹ã«é«˜ã„ãƒ»ä½ã„ç‰¹å¾´é‡ã‚’ç‰¹å®š
        for feature in feature_columns:
            cluster_mean = cluster_data[feature].mean()
            
            if cluster_mean > overall_means[feature] * 1.2:
                characteristics.append(f"{feature}ãŒé«˜ã„")
            elif cluster_mean < overall_means[feature] * 0.8:
                characteristics.append(f"{feature}ãŒä½ã„")
        
        return characteristics
    
    def _generate_cluster_insights(self, cluster_profiles: Dict[int, Dict]) -> List[str]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã®æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        # æœ€å¤§ãƒ»æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç‰¹å®š
        cluster_sizes = {k: v['size'] for k, v in cluster_profiles.items()}
        largest_cluster = max(cluster_sizes, key=cluster_sizes.get)
        smallest_cluster = min(cluster_sizes, key=cluster_sizes.get)
        
        insights.append(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼{largest_cluster}ãŒæœ€å¤§ï¼ˆ{cluster_sizes[largest_cluster]}è¨˜äº‹, {cluster_profiles[largest_cluster]['percentage']:.1f}%ï¼‰")
        insights.append(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼{smallest_cluster}ãŒæœ€å°ï¼ˆ{cluster_sizes[smallest_cluster]}è¨˜äº‹, {cluster_profiles[smallest_cluster]['percentage']:.1f}%ï¼‰")
        
        # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç‰¹å®š
        avg_pvs = {k: v['feature_means'].get('daily_pv', 0) for k, v in cluster_profiles.items()}
        best_cluster = max(avg_pvs, key=avg_pvs.get)
        
        insights.append(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼{best_cluster}ãŒæœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆå¹³å‡PV: {avg_pvs[best_cluster]:.0f}ï¼‰")
        insights.append(f"é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜äº‹ã®ç‰¹å¾´: {', '.join(cluster_profiles[best_cluster]['common_characteristics'])}")
        
        return insights
    
    def _generate_cluster_recommendations(self, cluster_profiles: Dict[int, Dict]) -> List[str]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã«åŸºã¥ãæ¨å¥¨äº‹é …"""
        recommendations = []
        
        # é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç‰¹å¾´ã‚’ä»–ã«é©ç”¨
        avg_pvs = {k: v['feature_means'].get('daily_pv', 0) for k, v in cluster_profiles.items()}
        best_cluster = max(avg_pvs, key=avg_pvs.get)
        best_characteristics = cluster_profiles[best_cluster]['common_characteristics']
        
        recommendations.append(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼{best_cluster}ã®ç‰¹å¾´ï¼ˆ{', '.join(best_characteristics)}ï¼‰ã‚’ä»–ã®è¨˜äº‹ã«ã‚‚é©ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        # ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æ”¹å–„ææ¡ˆ
        worst_cluster = min(avg_pvs, key=avg_pvs.get)
        worst_characteristics = cluster_profiles[worst_cluster]['common_characteristics']
        
        recommendations.append(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼{worst_cluster}ã®è¨˜äº‹ã¯æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ç‰¹ã«{', '.join(worst_characteristics)}ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸæˆ¦ç•¥ææ¡ˆ
        recommendations.append("ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æˆåŠŸè¦å› ã‚’çµ„ã¿åˆã‚ã›ãŸè¨˜äº‹æˆ¦ç•¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    # ============================================================
    # 4. æ™‚ç³»åˆ—åˆ†æ
    # ============================================================
    
    def time_series_analysis(
        self, 
        df: pd.DataFrame,
        date_column: str = 'publish_date',
        value_column: str = 'daily_pv',
        article_id: str = None
    ) -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—åˆ†æï¼ˆå­£ç¯€æ€§ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†è§£ï¼‰
        
        Args:
            df: åˆ†æãƒ‡ãƒ¼ã‚¿
            date_column: æ—¥ä»˜åˆ—
            value_column: åˆ†æå¯¾è±¡å€¤
            article_id: ç‰¹å®šè¨˜äº‹IDï¼ˆNoneã®å ´åˆã¯å…¨ä½“åˆ†æï¼‰
        """
        print(f"ğŸ“ˆ æ™‚ç³»åˆ—åˆ†æé–‹å§‹: {value_column}")
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        if article_id:
            analysis_df = df[df['article_id'] == article_id].copy()
        else:
            # å…¨è¨˜äº‹ã®æ—¥æ¬¡é›†è¨ˆ
            analysis_df = df.groupby(date_column)[value_column].sum().reset_index()
        
        analysis_df = analysis_df.sort_values(date_column)
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        ts_data = analysis_df.set_index(date_column)[value_column]
        
        # å­£ç¯€æ€§åˆ†è§£ï¼ˆé€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        if len(ts_data) >= 14:  # æœ€ä½2é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            decomposition = seasonal_decompose(ts_data, model='additive', period=7)
            
            trend = decomposition.trend.dropna()
            seasonal = decomposition.seasonal.dropna()
            residual = decomposition.resid.dropna()
        else:
            trend = seasonal = residual = ts_data
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        if len(trend) > 1:
            trend_slope, trend_intercept, trend_r, trend_p, trend_stderr = stats.linregress(
                range(len(trend)), trend.values
            )
            
            if trend_p < 0.05:
                if trend_slope > 0:
                    trend_direction = "ä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰"
                else:
                    trend_direction = "ä¸‹é™ãƒˆãƒ¬ãƒ³ãƒ‰"
            else:
                trend_direction = "æ˜ç¢ºãªãƒˆãƒ¬ãƒ³ãƒ‰ãªã—"
        else:
            trend_slope = trend_intercept = trend_r = trend_p = 0
            trend_direction = "ãƒ‡ãƒ¼ã‚¿ä¸è¶³"
        
        # å­£ç¯€æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        if len(seasonal) >= 7:
            weekday_patterns = {}
            for i in range(7):
                weekday_data = seasonal.iloc[i::7]
                if len(weekday_data) > 0:
                    weekday_patterns[i] = weekday_data.mean()
            
            best_weekday = max(weekday_patterns, key=weekday_patterns.get)
            worst_weekday = min(weekday_patterns, key=weekday_patterns.get)
        else:
            weekday_patterns = {}
            best_weekday = worst_weekday = 0
        
        # ç•°å¸¸å€¤æ¤œå‡º
        if len(residual) > 0:
            residual_mean = residual.mean()
            residual_std = residual.std()
            anomalies = residual[abs(residual - residual_mean) > 2 * residual_std]
        else:
            anomalies = pd.Series([])
        
        result = {
            'trend_analysis': {
                'direction': trend_direction,
                'slope': trend_slope,
                'r_squared': trend_r ** 2,
                'p_value': trend_p,
                'significance': trend_p < 0.05
            },
            'seasonal_patterns': {
                'weekday_effects': weekday_patterns,
                'best_weekday': best_weekday,
                'worst_weekday': worst_weekday,
                'seasonality_strength': seasonal.std() / ts_data.std() if len(seasonal) > 0 else 0
            },
            'anomaly_detection': {
                'anomaly_count': len(anomalies),
                'anomaly_dates': anomalies.index.tolist(),
                'anomaly_values': anomalies.values.tolist()
            },
            'summary_statistics': {
                'mean': ts_data.mean(),
                'std': ts_data.std(),
                'min': ts_data.min(),
                'max': ts_data.max(),
                'total_observations': len(ts_data)
            },
            'insights': self._generate_timeseries_insights(trend_direction, weekday_patterns, len(anomalies)),
            'recommendations': self._generate_timeseries_recommendations(trend_direction, best_weekday, worst_weekday)
        }
        
        print(f"âœ… æ™‚ç³»åˆ—åˆ†æå®Œäº†: {trend_direction}")
        return result
    
    def _generate_timeseries_insights(self, trend_direction: str, weekday_patterns: Dict, anomaly_count: int) -> List[str]:
        """æ™‚ç³»åˆ—åˆ†æã®æ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        insights.append(f"å…¨ä½“çš„ãªå‚¾å‘: {trend_direction}")
        
        if weekday_patterns:
            weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
            best_day = max(weekday_patterns, key=weekday_patterns.get)
            worst_day = min(weekday_patterns, key=weekday_patterns.get)
            
            insights.append(f"æœ€ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé«˜ã„æ›œæ—¥: {weekday_names[best_day]}æ›œæ—¥")
            insights.append(f"æœ€ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ã„æ›œæ—¥: {weekday_names[worst_day]}æ›œæ—¥")
        
        if anomaly_count > 0:
            insights.append(f"{anomaly_count}å€‹ã®ç•°å¸¸å€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        
        return insights
    
    def _generate_timeseries_recommendations(self, trend_direction: str, best_weekday: int, worst_weekday: int) -> List[str]:
        """æ™‚ç³»åˆ—åˆ†æã«åŸºã¥ãæ¨å¥¨äº‹é …"""
        recommendations = []
        
        weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        
        if "ä¸Šæ˜‡" in trend_direction:
            recommendations.append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®æˆ¦ç•¥ã‚’ç¶™ç¶šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        elif "ä¸‹é™" in trend_direction:
            recommendations.append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚æˆ¦ç•¥ã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™ã€‚")
        
        if best_weekday is not None:
            recommendations.append(f"{weekday_names[best_weekday]}æ›œæ—¥ã®å…¬é–‹ãƒ»ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        if worst_weekday is not None:
            recommendations.append(f"{weekday_names[worst_weekday]}æ›œæ—¥ã¯é¿ã‘ã‚‹ã‹ã€ç‰¹åˆ¥ãªæ–½ç­–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    # ============================================================
    # 5. çµ±åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    # ============================================================
    
    def generate_comprehensive_analysis_report(
        self, 
        articles_data: List[ArticlePerformanceData],
        target_variable: str = 'daily_pv'
    ) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("ğŸ“‹ åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
        print("=" * 50)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
        df = self.prepare_analysis_dataset(articles_data)
        df = self.encode_categorical_variables(df)
        
        # å„ç¨®åˆ†æå®Ÿè¡Œ
        regression_results = self.multiple_regression_analysis(df, target_variable)
        cluster_results = self.cluster_analysis(df)
        timeseries_results = self.time_series_analysis(df, value_column=target_variable)
        
        # çµ±åˆæ´å¯Ÿç”Ÿæˆ
        integrated_insights = self._generate_integrated_insights(
            regression_results, cluster_results, timeseries_results
        )
        
        # åŒ…æ‹¬çš„æ¨å¥¨äº‹é …
        comprehensive_recommendations = self._generate_comprehensive_recommendations(
            regression_results, cluster_results, timeseries_results
        )
        
        report = {
            'executive_summary': {
                'total_articles_analyzed': len(set(df['article_id'])),
                'analysis_period': f"{df['publish_date'].min()} ã€œ {df['publish_date'].max()}",
                'key_findings': integrated_insights[:5],
                'priority_recommendations': comprehensive_recommendations[:3]
            },
            'regression_analysis': regression_results,
            'cluster_analysis': cluster_results,
            'time_series_analysis': timeseries_results,
            'integrated_insights': integrated_insights,
            'comprehensive_recommendations': comprehensive_recommendations,
            'data_quality_assessment': self._assess_data_quality(df),
            'methodology_notes': self._generate_methodology_notes()
        }
        
        print("âœ… åŒ…æ‹¬çš„åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        return report
    
    def _generate_integrated_insights(self, regression_results: Dict, cluster_results: Dict, timeseries_results: Dict) -> List[str]:
        """çµ±åˆæ´å¯Ÿç”Ÿæˆ"""
        insights = []
        
        # å›å¸°åˆ†æã‹ã‚‰ã®ä¸»è¦çŸ¥è¦‹
        if regression_results['coefficients']:
            top_factor = max(regression_results['coefficients'], key=lambda x: abs(x['coefficient']))
            insights.append(f"æœ€ã‚‚é‡è¦ãªè¦å› : {top_factor['feature']}ï¼ˆä¿‚æ•°: {top_factor['coefficient']:.3f}ï¼‰")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‹ã‚‰ã®çŸ¥è¦‹
        if cluster_results['cluster_insights']:
            insights.extend(cluster_results['cluster_insights'][:2])
        
        # æ™‚ç³»åˆ—åˆ†æã‹ã‚‰ã®çŸ¥è¦‹
        if timeseries_results['insights']:
            insights.extend(timeseries_results['insights'][:2])
        
        # ç›¸äº’é–¢ä¿‚ã®æ´å¯Ÿ
        if regression_results['model_performance']['r2_test'] > 0.5 and cluster_results['silhouette_score'] > 0.3:
            insights.append("å›å¸°åˆ†æã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã®ä¸¡æ–¹ã§æœ‰æ„ãªçµæœãŒå¾—ã‚‰ã‚Œã¦ãŠã‚Šã€åˆ†æã®ä¿¡é ¼æ€§ãŒé«˜ã„ã§ã™ã€‚")
        
        return insights
    
    def _generate_comprehensive_recommendations(self, regression_results: Dict, cluster_results: Dict, timeseries_results: Dict) -> List[str]:
        """åŒ…æ‹¬çš„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # å›å¸°åˆ†æã‹ã‚‰ã®æ¨å¥¨äº‹é …
        if regression_results['recommendations']:
            recommendations.extend(regression_results['recommendations'][:2])
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æã‹ã‚‰ã®æ¨å¥¨äº‹é …
        if cluster_results['recommendations']:
            recommendations.extend(cluster_results['recommendations'][:2])
        
        # æ™‚ç³»åˆ—åˆ†æã‹ã‚‰ã®æ¨å¥¨äº‹é …
        if timeseries_results['recommendations']:
            recommendations.extend(timeseries_results['recommendations'][:2])
        
        # çµ±åˆçš„ãªæ¨å¥¨äº‹é …
        recommendations.append("å„åˆ†ææ‰‹æ³•ã®çµæœã‚’ç·åˆçš„ã«æ¤œè¨ã—ã€æ®µéšçš„ãªæ”¹å–„è¨ˆç”»ã‚’ç­–å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        recommendations.append("ç¶™ç¶šçš„ãªãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®æ§‹ç¯‰ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªè¨˜äº‹æˆ¦ç•¥ã‚’å®Ÿç¾ã—ã¦ãã ã•ã„ã€‚")
        
        return recommendations
    
    def _assess_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""
        return {
            'completeness': {
                'total_records': len(df),
                'missing_values': df.isnull().sum().to_dict(),
                'completeness_rate': (1 - df.isnull().sum() / len(df)).to_dict()
            },
            'consistency': {
                'duplicate_records': df.duplicated().sum(),
                'data_types': df.dtypes.to_dict()
            },
            'validity': {
                'negative_values': (df.select_dtypes(include=[np.number]) < 0).sum().to_dict(),
                'outliers_detected': self._detect_outliers(df)
            }
        }
    
    def _detect_outliers(self, df: pd.DataFrame) -> Dict[str, int]:
        """å¤–ã‚Œå€¤æ¤œå‡º"""
        outliers = {}
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            outliers[col] = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
        
        return outliers
    
    def _generate_methodology_notes(self) -> List[str]:
        """æ–¹æ³•è«–ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        return [
            "å·®åˆ†ã®å·®åˆ†æ³•ï¼ˆDIDï¼‰: å‡¦ç½®ç¾¤ã¨å¯¾ç…§ç¾¤ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’æ¯”è¼ƒã—ã¦å› æœåŠ¹æœã‚’æ¨å®š",
            "é‡å›å¸°åˆ†æ: è¤‡æ•°ã®èª¬æ˜å¤‰æ•°ãŒã‚¢ã‚¦ãƒˆã‚«ãƒ ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®šé‡åŒ–",
            "ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ: K-meansæ³•ã‚’ç”¨ã„ã¦è¨˜äº‹ã‚’ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ†é¡",
            "æ™‚ç³»åˆ—åˆ†æ: å­£ç¯€æ€§åˆ†è§£ã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å­£ç¯€æ€§ãƒ»ç•°å¸¸å€¤ã‚’ç‰¹å®š",
            "å¤šé‡å…±ç·šæ€§: VIFå€¤ã«ã‚ˆã‚‹èª¬æ˜å¤‰æ•°é–“ã®ç›¸é–¢é–¢ä¿‚ã‚’è©•ä¾¡",
            "çµ±è¨ˆçš„æœ‰æ„æ€§: på€¤0.05ã‚’åŸºæº–ã¨ã—ãŸçµ±è¨ˆçš„æ¤œå®šã‚’å®Ÿæ–½"
        ]


# ============================================================
# ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨ä¾‹
# ============================================================

if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã®ä½¿ç”¨æ™‚ã¯DBã‹ã‚‰å–å¾—ï¼‰
    sample_articles = [
        ArticlePerformanceData(
            article_id="article_001",
            title="èª•ç”ŸèŠ±å®Œå…¨ã‚¬ã‚¤ãƒ‰",
            publish_date=datetime(2024, 1, 1),
            word_count=5000,
            keyword_density=0.02,
            seo_score=85,
            tone_manner="formal",
            author="author_a",
            category="gardening",
            pv_daily=[100, 150, 200, 180, 220, 190, 160] * 10,
            unique_users=[80, 120, 160, 144, 176, 152, 128] * 10,
            avg_time_on_page=[3.5, 4.0, 4.2, 3.8, 4.5, 4.1, 3.7] * 10,
            bounce_rate=[0.3, 0.25, 0.2, 0.28, 0.18, 0.22, 0.32] * 10,
            social_shares=[5, 8, 12, 10, 15, 11, 7] * 10,
            conversions=[2, 3, 5, 4, 6, 4, 3] * 10,
            search_impressions=[1000, 1200, 1500, 1300, 1600, 1400, 1100] * 10,
            search_clicks=[50, 70, 90, 80, 100, 85, 65] * 10,
            avg_position=[8.5, 7.2, 6.1, 6.8, 5.9, 6.5, 7.8] * 10,
            tags=["images", "comprehensive"],
            promotion_activities=["social_media", "email"],
            external_events=[]
        )
    ]
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = AdvancedStatisticalAnalyzer()
    report = analyzer.generate_comprehensive_analysis_report(sample_articles)
    
    print("\nğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:")
    print(f"åˆ†æè¨˜äº‹æ•°: {report['executive_summary']['total_articles_analyzed']}")
    print(f"ä¸»è¦çŸ¥è¦‹: {report['executive_summary']['key_findings'][0]}")
    print(f"å„ªå…ˆæ¨å¥¨äº‹é …: {report['executive_summary']['priority_recommendations'][0]}")