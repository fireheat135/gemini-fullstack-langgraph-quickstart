#!/usr/bin/env python3
"""
TDD ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼

è‡ªå·±å›å¸°ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
"""

import os
import time
import asyncio
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path

from .coverage_monitor import CoverageMonitor
from .quality_evaluator import QualityEvaluator
from .convergence_checker import ConvergenceChecker


@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    test_name: str
    passed: bool
    execution_time: float
    coverage: float
    quality_score: float
    issues: List[str]
    suggestions: List[str]
    metadata: Dict[str, Any] = None


@dataclass
class TDDLoopResult:
    """TDDãƒ«ãƒ¼ãƒ—çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    iterations: int
    converged: bool
    final_metrics: Dict[str, float]
    improvement_trend: Dict[str, Any]
    recommendations: List[str]
    test_results: List[TestResult]
    execution_time: float


class TDDTestRunner:
    """
    ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºã®ãƒ¡ã‚¤ãƒ³ãƒ©ãƒ³ãƒŠãƒ¼
    
    è‡ªå·±å›å¸°ãƒ«ãƒ¼ãƒ—ã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€å“è³ªæŒ‡æ¨™ãŒé–¾å€¤ã«é”ã™ã‚‹ã¾ã§æ”¹å–„ã‚’ç¶™ç¶š
    """
    
    def __init__(
        self,
        coverage_threshold: float = 0.8,
        quality_threshold: float = 0.9,
        max_iterations: int = 5,
        config_path: Optional[str] = None
    ):
        self.coverage_threshold = coverage_threshold
        self.quality_threshold = quality_threshold
        self.max_iterations = max_iterations
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.coverage_monitor = CoverageMonitor(threshold=coverage_threshold)
        self.quality_evaluator = QualityEvaluator()
        self.convergence_checker = ConvergenceChecker(
            thresholds={
                'coverage': coverage_threshold,
                'quality_score': quality_threshold,
                'success_rate': 0.8
            }
        )
        
        # å®Ÿè¡Œå±¥æ­´
        self.loop_history = []
        self.current_iteration = 0
        
    async def run_self_recursive_loop(self, test_modules: List[str]) -> TDDLoopResult:
        """
        è‡ªå·±å›å¸°ãƒ†ã‚¹ãƒˆãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ
        
        Args:
            test_modules: å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            TDDLoopResult: ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œçµæœ
        """
        print("ğŸ”„ TDDè‡ªå·±å›å¸°ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        print("="*60)
        
        start_time = time.time()
        
        for iteration in range(self.max_iterations):
            self.current_iteration = iteration + 1
            print(f"\nğŸ“ åå¾© {self.current_iteration}/{self.max_iterations}")
            
            # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
            iteration_results = await self._run_test_iteration(test_modules)
            self.loop_history.append(iteration_results)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            metrics = self._calculate_iteration_metrics(iteration_results)
            
            # åæŸåˆ¤å®š
            convergence = self.convergence_checker.check_convergence(metrics)
            
            self._print_iteration_summary(metrics, convergence)
            
            if convergence.converged:
                print(f"âœ… åæŸå®Œäº†ï¼({iteration + 1}å›ç›®ã§åŸºæº–é”æˆ)")
                break
            else:
                print(f"ğŸ”„ æ”¹å–„ãŒå¿…è¦: {convergence.reason}")
                # è‡ªå‹•æ”¹å–„é©ç”¨
                await self._apply_improvements(convergence.suggestions)
        
        total_time = time.time() - start_time
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        final_result = self._generate_final_result(total_time)
        
        print(f"\nğŸ‰ TDDãƒ«ãƒ¼ãƒ—å®Œäº† (å®Ÿè¡Œæ™‚é–“: {total_time:.2f}s)")
        return final_result
    
    async def _run_test_iteration(self, test_modules: List[str]) -> List[TestResult]:
        """å˜ä¸€åå¾©ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        results = []
        
        for module in test_modules:
            try:
                result = await self._execute_test_module(module)
                results.append(result)
                status = "âœ…" if result.passed else "âŒ"
                print(f"   {status} {result.test_name}: Q={result.quality_score:.2f}")
            except Exception as e:
                error_result = TestResult(
                    test_name=module,
                    passed=False,
                    execution_time=0.0,
                    coverage=0.0,
                    quality_score=0.0,
                    issues=[str(e)],
                    suggestions=["ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„"]
                )
                results.append(error_result)
                print(f"   âŒ {module}: Exception - {e}")
        
        return results
    
    async def _execute_test_module(self, module: str) -> TestResult:
        """å€‹åˆ¥ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Ÿè¡Œ"""
        start_time = time.time()
        
        # ã“ã“ã§å®Ÿéš›ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
        # ä¾‹: pytest, unittest, ã‚«ã‚¹ã‚¿ãƒ ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼ãªã©
        
        # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ç½®ãæ›ãˆï¼‰
        await asyncio.sleep(0.1)  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        
        execution_time = time.time() - start_time
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
        coverage = self.coverage_monitor.measure_coverage(module)
        
        # å“è³ªè©•ä¾¡
        quality_score = self.quality_evaluator.evaluate_module(module)
        
        return TestResult(
            test_name=module,
            passed=quality_score > 0.7,
            execution_time=execution_time,
            coverage=coverage,
            quality_score=quality_score,
            issues=[],
            suggestions=[]
        )
    
    def _calculate_iteration_metrics(self, results: List[TestResult]) -> Dict[str, float]:
        """åå¾©ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        if not results:
            return {'coverage': 0.0, 'quality_score': 0.0, 'success_rate': 0.0}
        
        total_coverage = sum(r.coverage for r in results) / len(results)
        total_quality = sum(r.quality_score for r in results) / len(results)
        success_rate = sum(1 for r in results if r.passed) / len(results)
        
        return {
            'coverage': total_coverage,
            'quality_score': total_quality,
            'success_rate': success_rate
        }
    
    def _print_iteration_summary(self, metrics: Dict[str, float], convergence):
        """åå¾©çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        print(f"   ã‚«ãƒãƒ¬ãƒƒã‚¸: {metrics['coverage']:.1%}")
        print(f"   å“è³ªã‚¹ã‚³ã‚¢: {metrics['quality_score']:.1%}")
        print(f"   æˆåŠŸç‡: {metrics['success_rate']:.1%}")
        
        if not convergence.converged:
            print(f"ğŸ” æ”¹å–„ç‚¹:")
            for suggestion in convergence.suggestions:
                print(f"   â€¢ {suggestion}")
    
    async def _apply_improvements(self, suggestions: List[str]):
        """è‡ªå‹•æ”¹å–„ã®é©ç”¨"""
        print(f"ğŸ”§ æ”¹å–„ã‚’é©ç”¨ä¸­:")
        for suggestion in suggestions:
            print(f"   â€¢ {suggestion}")
        
        # å®Ÿéš›ã®æ”¹å–„ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«å®Ÿè£…
        await asyncio.sleep(0.5)  # æ”¹å–„å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    
    def _generate_final_result(self, execution_time: float) -> TDDLoopResult:
        """æœ€çµ‚çµæœã®ç”Ÿæˆ"""
        final_iteration = self.loop_history[-1] if self.loop_history else []
        final_metrics = self._calculate_iteration_metrics(final_iteration)
        
        return TDDLoopResult(
            iterations=len(self.loop_history),
            converged=final_metrics.get('quality_score', 0) >= self.quality_threshold,
            final_metrics=final_metrics,
            improvement_trend=self._analyze_improvement_trend(),
            recommendations=self._generate_recommendations(final_iteration),
            test_results=final_iteration,
            execution_time=execution_time
        )
    
    def _analyze_improvement_trend(self) -> Dict[str, Any]:
        """æ”¹å–„å‚¾å‘ã®åˆ†æ"""
        if len(self.loop_history) < 2:
            return {'trend': 'insufficient_data'}
        
        metrics_over_time = [
            self._calculate_iteration_metrics(results) 
            for results in self.loop_history
        ]
        
        coverage_trend = [m['coverage'] for m in metrics_over_time]
        quality_trend = [m['quality_score'] for m in metrics_over_time]
        
        return {
            'coverage_improvement': coverage_trend[-1] - coverage_trend[0],
            'quality_improvement': quality_trend[-1] - quality_trend[0],
            'convergence_speed': len(self.loop_history),
            'stability': self._calculate_stability(metrics_over_time)
        }
    
    def _calculate_stability(self, metrics_over_time: List[Dict[str, float]]) -> float:
        """å®‰å®šæ€§ã®è¨ˆç®—"""
        if len(metrics_over_time) < 3:
            return 0.0
        
        recent_metrics = metrics_over_time[-3:]
        variances = []
        
        for key in ['coverage', 'quality_score', 'success_rate']:
            values = [m[key] for m in recent_metrics]
            mean = sum(values) / len(values)
            variance = sum((v - mean)**2 for v in values) / len(values)
            variances.append(variance)
        
        return 1.0 - sum(variances) / len(variances)
    
    def _generate_recommendations(self, results: List[TestResult]) -> List[str]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        if not results:
            recommendations.append("åŸºæœ¬ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã¦ãã ã•ã„")
            return recommendations
        
        failed_tests = [r for r in results if not r.passed]
        if failed_tests:
            recommendations.append(f"{len(failed_tests)}å€‹ã®å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’å„ªå…ˆçš„ã«ä¿®æ­£")
        
        low_coverage_tests = [r for r in results if r.coverage < self.coverage_threshold]
        if low_coverage_tests:
            recommendations.append("ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒä½ã„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å¼·åŒ–")
        
        low_quality_tests = [r for r in results if r.quality_score < self.quality_threshold]
        if low_quality_tests:
            recommendations.append("å“è³ªã‚¹ã‚³ã‚¢ãŒä½ã„ãƒ†ã‚¹ãƒˆã®æ”¹å–„")
        
        if all(r.passed for r in results):
            recommendations.append("å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼è¿½åŠ ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚’æ¤œè¨")
        
        return recommendations