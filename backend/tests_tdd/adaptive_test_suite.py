#!/usr/bin/env python3
"""
ğŸ§  Adaptive Test Suite - Ultrathink Implementation
è‡ªå·±é€²åŒ–å‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã¨ãƒªã‚¹ã‚¯è©•ä¾¡ã«åŸºã¥ãçŸ¥çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import statistics
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BusinessPriority(Enum):
    """ãƒ“ã‚¸ãƒã‚¹å„ªå…ˆåº¦"""
    CRITICAL = "critical"      # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ 9-10ç‚¹
    HIGH = "high"             # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ 7-8ç‚¹  
    MEDIUM = "medium"         # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ 5-6ç‚¹
    LOW = "low"               # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ 3-4ç‚¹


class RiskLevel(Enum):
    """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"""
    EXTREME = "extreme"       # å¤–éƒ¨ä¾å­˜åº¦ 9-10ç‚¹
    HIGH = "high"            # å¤–éƒ¨ä¾å­˜åº¦ 7-8ç‚¹
    MEDIUM = "medium"        # å¤–éƒ¨ä¾å­˜åº¦ 5-6ç‚¹
    LOW = "low"              # å¤–éƒ¨ä¾å­˜åº¦ 3-4ç‚¹


@dataclass
class TestScenario:
    """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªå®šç¾©"""
    name: str
    component: str
    business_priority: BusinessPriority
    risk_level: RiskLevel
    complexity: int  # 1-10
    estimated_duration: float  # seconds
    success_criteria: Dict[str, Any]
    dependencies: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœ"""
    scenario: TestScenario
    passed: bool
    execution_time: float
    quality_score: float
    performance_metrics: Dict[str, float]
    business_impact: float
    issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)


class AdaptiveTestSuite:
    """
    è‡ªå·±é€²åŒ–å‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
    
    ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã¨ãƒªã‚¹ã‚¯è©•ä¾¡ã«åŸºã¥ã„ã¦ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’
    å‹•çš„ã«ç”Ÿæˆãƒ»å„ªå…ˆåº¦ä»˜ã‘ãƒ»å®Ÿè¡Œ
    """
    
    def __init__(self):
        self.business_value_weights = {
            'ai_service_manager': 1.0,      # æœ€é‡è¦
            'content_generator': 0.9,       # ã‚³ã‚¢æ©Ÿèƒ½
            'langgraph_integration': 0.9,   # å·®åˆ¥åŒ–è¦å› 
            'keyword_analyzer': 0.8,        # SEOåŸºç›¤
            'content_management': 0.7,      # å“è³ªä¿è¨¼
            'competitor_analyzer': 0.6,     # å¸‚å ´åˆ†æ
            'trend_analyzer': 0.5           # æ”¯æ´æ©Ÿèƒ½
        }
        
        self.risk_factors = {
            'external_api_dependency': 0.9,
            'async_workflow_complexity': 0.8,
            'ai_response_variability': 0.8,
            'performance_bottleneck': 0.7,
            'data_consistency': 0.6
        }
        
        self.execution_history = []
        self.quality_trends = {}
        self.performance_baselines = {}
        
    async def generate_priority_test_suite(self) -> List[TestScenario]:
        """ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã¨ãƒªã‚¹ã‚¯è©•ä¾¡ã«åŸºã¥ããƒ†ã‚¹ãƒˆå„ªå…ˆåº¦ä»˜ã‘"""
        
        logger.info("ğŸ§  Generating adaptive test suite based on business value analysis...")
        
        test_scenarios = []
        
        # Critical Path Testing (æœ€å„ªå…ˆ)
        critical_scenarios = await self._generate_critical_path_tests()
        test_scenarios.extend(critical_scenarios)
        
        # Risk-Based Testing (é«˜ãƒªã‚¹ã‚¯è¦å› )
        risk_scenarios = await self._generate_risk_based_tests()
        test_scenarios.extend(risk_scenarios)
        
        # Performance-Critical Testing
        performance_scenarios = await self._generate_performance_tests()
        test_scenarios.extend(performance_scenarios)
        
        # Business Logic Validation
        business_scenarios = await self._generate_business_logic_tests()
        test_scenarios.extend(business_scenarios)
        
        # å„ªå…ˆåº¦ä»˜ã‘ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
        prioritized_scenarios = self._prioritize_and_schedule(test_scenarios)
        
        logger.info(f"âœ… Generated {len(prioritized_scenarios)} adaptive test scenarios")
        return prioritized_scenarios
    
    async def _generate_critical_path_tests(self) -> List[TestScenario]:
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        
        scenarios = []
        
        # AIServiceManager - æœ€é«˜ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤
        scenarios.append(TestScenario(
            name="ai_service_complete_failover_resilience",
            component="ai_service_manager",
            business_priority=BusinessPriority.CRITICAL,
            risk_level=RiskLevel.EXTREME,
            complexity=9,
            estimated_duration=45.0,
            success_criteria={
                'failover_time': 2.0,      # 2ç§’ä»¥å†…
                'success_rate': 0.99,      # 99%æˆåŠŸç‡
                'data_integrity': 1.0      # 100%ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            },
            dependencies=['gemini_api', 'claude_api', 'openai_api']
        ))
        
        # DeepResearchContentGenerator - ã‚³ã‚¢æ©Ÿèƒ½
        scenarios.append(TestScenario(
            name="content_generation_end_to_end_quality",
            component="content_generator", 
            business_priority=BusinessPriority.CRITICAL,
            risk_level=RiskLevel.HIGH,
            complexity=8,
            estimated_duration=60.0,
            success_criteria={
                'content_quality_score': 85,    # 85ç‚¹ä»¥ä¸Š
                'seo_optimization_score': 80,   # 80ç‚¹ä»¥ä¸Š
                'generation_time': 30.0,        # 30ç§’ä»¥å†…
                'factual_accuracy': 0.95        # 95%æ­£ç¢ºæ€§
            },
            dependencies=['ai_service_manager', 'keyword_analyzer']
        ))
        
        # LangGraph Integration - å·®åˆ¥åŒ–è¦å› 
        scenarios.append(TestScenario(
            name="langgraph_workflow_consistency",
            component="langgraph_integration",
            business_priority=BusinessPriority.CRITICAL,
            risk_level=RiskLevel.HIGH,
            complexity=9,
            estimated_duration=40.0,
            success_criteria={
                'workflow_completion_rate': 0.98,  # 98%å®Œäº†ç‡
                'step_failure_recovery': 0.95,     # 95%å›å¾©ç‡
                'data_flow_integrity': 1.0         # 100%ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
            },
            dependencies=['google_search_api', 'ai_service_manager']
        ))
        
        return scenarios
    
    async def _generate_risk_based_tests(self) -> List[TestScenario]:
        """ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        
        scenarios = []
        
        # å¤–éƒ¨APIä¾å­˜ãƒªã‚¹ã‚¯
        scenarios.append(TestScenario(
            name="external_api_cascade_failure_recovery",
            component="external_dependencies",
            business_priority=BusinessPriority.HIGH,
            risk_level=RiskLevel.EXTREME,
            complexity=8,
            estimated_duration=30.0,
            success_criteria={
                'cascade_prevention': 1.0,     # 100%ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰éšœå®³é˜²æ­¢
                'recovery_time': 5.0,          # 5ç§’ä»¥å†…å›å¾©
                'fallback_success': 0.95       # 95%ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ
            },
            dependencies=['all_external_apis']
        ))
        
        # éåŒæœŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è¤‡é›‘æ€§ãƒªã‚¹ã‚¯
        scenarios.append(TestScenario(
            name="async_workflow_race_condition_detection",
            component="async_workflows",
            business_priority=BusinessPriority.HIGH,
            risk_level=RiskLevel.HIGH,
            complexity=7,
            estimated_duration=25.0,
            success_criteria={
                'race_condition_detection': 1.0,   # 100%æ¤œå‡º
                'deadlock_prevention': 1.0,        # 100%ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯é˜²æ­¢
                'data_consistency': 1.0            # 100%ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§
            }
        ))
        
        return scenarios
    
    async def _generate_performance_tests(self) -> List[TestScenario]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        
        scenarios = []
        
        # é«˜è² è·æ™‚æ€§èƒ½ãƒ†ã‚¹ãƒˆ
        scenarios.append(TestScenario(
            name="high_load_concurrent_content_generation",
            component="performance",
            business_priority=BusinessPriority.HIGH,
            risk_level=RiskLevel.MEDIUM,
            complexity=7,
            estimated_duration=120.0,  # 2åˆ†é–“è² è·ãƒ†ã‚¹ãƒˆ
            success_criteria={
                'concurrent_users': 50,        # 50åŒæ™‚ãƒ¦ãƒ¼ã‚¶ãƒ¼
                'avg_response_time': 30.0,     # å¹³å‡30ç§’ä»¥å†…
                'success_rate': 0.95,          # 95%æˆåŠŸç‡
                'memory_usage': 0.8            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡80%ä»¥ä¸‹
            }
        ))
        
        return scenarios
    
    async def _generate_business_logic_tests(self) -> List[TestScenario]:
        """ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        
        scenarios = []
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªä¸€è²«æ€§
        scenarios.append(TestScenario(
            name="content_quality_consistency_validation",
            component="content_quality",
            business_priority=BusinessPriority.HIGH,
            risk_level=RiskLevel.MEDIUM,
            complexity=6,
            estimated_duration=180.0,  # 3åˆ†é–“ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ
            success_criteria={
                'quality_standard_deviation': 5.0,  # æ¨™æº–åå·®5ä»¥ä¸‹
                'min_quality_score': 80,            # æœ€ä½80ç‚¹
                'consistency_rate': 0.95             # 95%ä¸€è²«æ€§
            }
        ))
        
        return scenarios
    
    def _prioritize_and_schedule(self, scenarios: List[TestScenario]) -> List[TestScenario]:
        """ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®å„ªå…ˆåº¦ä»˜ã‘ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°"""
        
        def calculate_priority_score(scenario: TestScenario) -> float:
            """å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
            
            # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤é‡ã¿
            business_weight = self.business_value_weights.get(scenario.component, 0.3)
            
            # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«é‡ã¿
            risk_weights = {
                RiskLevel.EXTREME: 1.0,
                RiskLevel.HIGH: 0.8,
                RiskLevel.MEDIUM: 0.6,
                RiskLevel.LOW: 0.4
            }
            risk_weight = risk_weights[scenario.risk_level]
            
            # ãƒ“ã‚¸ãƒã‚¹å„ªå…ˆåº¦é‡ã¿
            priority_weights = {
                BusinessPriority.CRITICAL: 1.0,
                BusinessPriority.HIGH: 0.8,
                BusinessPriority.MEDIUM: 0.6,
                BusinessPriority.LOW: 0.4
            }
            priority_weight = priority_weights[scenario.business_priority]
            
            # è¤‡é›‘åº¦ã«ã‚ˆã‚‹èª¿æ•´ï¼ˆè¤‡é›‘ã™ãã‚‹ãƒ†ã‚¹ãƒˆã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹ï¼‰
            complexity_adjustment = max(0.3, 1.0 - (scenario.complexity * 0.05))
            
            # ç·åˆã‚¹ã‚³ã‚¢
            total_score = (
                business_weight * 0.4 +
                risk_weight * 0.3 +
                priority_weight * 0.2 +
                complexity_adjustment * 0.1
            )
            
            return total_score
        
        # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        prioritized = sorted(
            scenarios,
            key=calculate_priority_score,
            reverse=True
        )
        
        logger.info("ğŸ“Š Test prioritization completed:")
        for i, scenario in enumerate(prioritized[:5], 1):
            score = calculate_priority_score(scenario)
            logger.info(f"  {i}. {scenario.name} (score: {score:.3f})")
        
        return prioritized


class SmartTestExecutor:
    """
    ã‚¹ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
    
    å®Ÿè¡Œçµæœã‚’å­¦ç¿’ã—ã¦ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã‚’æœ€é©åŒ–
    """
    
    def __init__(self, adaptive_suite: AdaptiveTestSuite):
        self.adaptive_suite = adaptive_suite
        self.execution_patterns = {}
        self.quality_history = []
        
    async def execute_adaptive_test_cycle(self) -> Dict[str, Any]:
        """é©å¿œçš„ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ"""
        
        logger.info("ğŸš€ Starting adaptive test execution cycle...")
        
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆç”Ÿæˆ
        scenarios = await self.adaptive_suite.generate_priority_test_suite()
        
        # å®Ÿè¡Œçµæœåé›†
        results = []
        total_start_time = time.time()
        
        for scenario in scenarios:
            logger.info(f"ğŸ§ª Executing: {scenario.name}")
            
            result = await self._execute_scenario(scenario)
            results.append(result)
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å“è³ªè©•ä¾¡
            if not result.passed:
                logger.warning(f"âŒ Failed: {scenario.name} - {result.issues}")
            else:
                logger.info(f"âœ… Passed: {scenario.name} (Quality: {result.quality_score:.2f})")
        
        total_execution_time = time.time() - total_start_time
        
        # å®Ÿè¡Œã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        summary = self._generate_execution_summary(results, total_execution_time)
        
        # å­¦ç¿’ã¨æœ€é©åŒ–
        await self._learn_and_optimize(results)
        
        logger.info(f"ğŸ‰ Adaptive test cycle completed in {total_execution_time:.2f}s")
        
        return summary
    
    async def _execute_scenario(self, scenario: TestScenario) -> TestResult:
        """å€‹åˆ¥ã‚·ãƒŠãƒªã‚ªå®Ÿè¡Œ"""
        
        start_time = time.time()
        
        try:
            # ã‚·ãƒŠãƒªã‚ªã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå®Ÿè¡Œ
            if 'ai_service' in scenario.component:
                test_result = await self._execute_ai_service_test(scenario)
            elif 'content_generator' in scenario.component:
                test_result = await self._execute_content_generation_test(scenario)
            elif 'performance' in scenario.component:
                test_result = await self._execute_performance_test(scenario)
            else:
                test_result = await self._execute_generic_test(scenario)
            
            execution_time = time.time() - start_time
            
            return TestResult(
                scenario=scenario,
                passed=test_result['passed'],
                execution_time=execution_time,
                quality_score=test_result.get('quality_score', 0.0),
                performance_metrics=test_result.get('performance_metrics', {}),
                business_impact=test_result.get('business_impact', 0.0),
                issues=test_result.get('issues', []),
                recommendations=test_result.get('recommendations', [])
            )
            
        except Exception as e:
            execution_time = time.time() - start_time
            
            return TestResult(
                scenario=scenario,
                passed=False,
                execution_time=execution_time,
                quality_score=0.0,
                performance_metrics={},
                business_impact=0.0,
                issues=[str(e)],
                recommendations=['Check test implementation and dependencies']
            )
    
    async def _execute_ai_service_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """AI ã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        # ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè£…ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å®Ÿéš›ã®AIã‚µãƒ¼ãƒ“ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼‰
        await asyncio.sleep(0.5)  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        return {
            'passed': True,
            'quality_score': 85.0,
            'performance_metrics': {
                'response_time': 1.5,
                'success_rate': 0.98,
                'failover_time': 1.2
            },
            'business_impact': 9.5,
            'recommendations': ['Monitor API rate limits closely']
        }
    
    async def _execute_content_generation_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        await asyncio.sleep(1.0)  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        return {
            'passed': True,
            'quality_score': 88.0,
            'performance_metrics': {
                'generation_time': 25.0,
                'content_quality': 87.0,
                'seo_score': 82.0
            },
            'business_impact': 9.0,
            'recommendations': ['Consider caching for repeated requests']
        }
    
    async def _execute_performance_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        await asyncio.sleep(2.0)  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        return {
            'passed': True,
            'quality_score': 82.0,
            'performance_metrics': {
                'avg_response_time': 28.0,
                'concurrent_users': 45,
                'success_rate': 0.96,
                'memory_usage': 0.75
            },
            'business_impact': 8.5
        }
    
    async def _execute_generic_test(self, scenario: TestScenario) -> Dict[str, Any]:
        """æ±ç”¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        
        await asyncio.sleep(0.3)  # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        
        return {
            'passed': True,
            'quality_score': 80.0,
            'performance_metrics': {},
            'business_impact': 7.0
        }
    
    def _generate_execution_summary(self, results: List[TestResult], 
                                  total_time: float) -> Dict[str, Any]:
        """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        
        passed_count = sum(1 for r in results if r.passed)
        total_count = len(results)
        
        avg_quality = statistics.mean([r.quality_score for r in results]) if results else 0.0
        avg_business_impact = statistics.mean([r.business_impact for r in results]) if results else 0.0
        
        critical_failures = [
            r for r in results 
            if not r.passed and r.scenario.business_priority == BusinessPriority.CRITICAL
        ]
        
        summary = {
            'execution_time': total_time,
            'total_tests': total_count,
            'passed_tests': passed_count,
            'success_rate': passed_count / total_count if total_count > 0 else 0.0,
            'average_quality_score': avg_quality,
            'average_business_impact': avg_business_impact,
            'critical_failures': len(critical_failures),
            'recommendations': self._generate_summary_recommendations(results)
        }
        
        return summary
    
    def _generate_summary_recommendations(self, results: List[TestResult]) -> List[str]:
        """ã‚µãƒãƒªãƒ¼æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        
        recommendations = []
        
        # å¤±æ•—ã—ãŸã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ†ã‚¹ãƒˆã®åˆ†æ
        critical_failures = [
            r for r in results 
            if not r.passed and r.scenario.business_priority == BusinessPriority.CRITICAL
        ]
        
        if critical_failures:
            recommendations.append(
                f"âš ï¸  {len(critical_failures)} critical tests failed - immediate attention required"
            )
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®æ¤œå‡º
        slow_tests = [r for r in results if r.execution_time > 60.0]
        if slow_tests:
            recommendations.append(
                f"ğŸŒ {len(slow_tests)} tests are running slowly - consider optimization"
            )
        
        # å“è³ªã‚¹ã‚³ã‚¢ã®åˆ†æ
        low_quality_tests = [r for r in results if r.quality_score < 70.0]
        if low_quality_tests:
            recommendations.append(
                f"ğŸ“‰ {len(low_quality_tests)} tests have low quality scores - review implementation"
            )
        
        return recommendations
    
    async def _learn_and_optimize(self, results: List[TestResult]):
        """å®Ÿè¡Œçµæœã‹ã‚‰å­¦ç¿’ã—ã¦æœ€é©åŒ–"""
        
        # å“è³ªå±¥æ­´ã®æ›´æ–°
        current_avg_quality = statistics.mean([r.quality_score for r in results])
        self.quality_history.append(current_avg_quality)
        
        # å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
        for result in results:
            component = result.scenario.component
            if component not in self.execution_patterns:
                self.execution_patterns[component] = []
            
            self.execution_patterns[component].append({
                'quality_score': result.quality_score,
                'execution_time': result.execution_time,
                'passed': result.passed
            })
        
        # æœ€é©åŒ–ã®å®Ÿè¡Œ
        if len(self.quality_history) >= 3:
            await self._optimize_test_strategy()
    
    async def _optimize_test_strategy(self):
        """ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®æœ€é©åŒ–"""
        
        # å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ
        recent_quality = self.quality_history[-3:]
        if all(q < recent_quality[0] for q in recent_quality[1:]):
            logger.warning("ğŸ“‰ Quality degradation detected - adjusting test strategy")
            # ã‚ˆã‚Šå³ã—ã„ãƒ†ã‚¹ãƒˆåŸºæº–ã‚’é©ç”¨
        
        # å®Ÿè¡Œæ™‚é–“ã®æœ€é©åŒ–
        for component, patterns in self.execution_patterns.items():
            avg_time = statistics.mean([p['execution_time'] for p in patterns])
            if avg_time > 30.0:
                logger.info(f"â° Optimizing execution time for {component}")


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¾‹
async def run_adaptive_test_demonstration():
    """é©å¿œçš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("ğŸ§  Starting Ultrathink Adaptive Test Suite Demonstration")
    print("=" * 60)
    
    # é©å¿œçš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆåˆæœŸåŒ–
    adaptive_suite = AdaptiveTestSuite()
    executor = SmartTestExecutor(adaptive_suite)
    
    # é©å¿œçš„ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ
    summary = await executor.execute_adaptive_test_cycle()
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“Š Execution Summary:")
    print(f"   Total Tests: {summary['total_tests']}")
    print(f"   Success Rate: {summary['success_rate']:.1%}")
    print(f"   Average Quality: {summary['average_quality_score']:.1f}")
    print(f"   Business Impact: {summary['average_business_impact']:.1f}")
    print(f"   Execution Time: {summary['execution_time']:.2f}s")
    
    if summary['recommendations']:
        print("\nğŸ’¡ Recommendations:")
        for rec in summary['recommendations']:
            print(f"   â€¢ {rec}")
    
    print("\nâœ… Adaptive test demonstration completed!")


if __name__ == "__main__":
    asyncio.run(run_adaptive_test_demonstration())