#!/usr/bin/env python3
"""
ğŸ§  Critical Path Tests - Content Generation Engine  
ã‚³ã‚¢æ©Ÿèƒ½ (ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ 9/10) ã®åŒ…æ‹¬çš„å“è³ªæ¤œè¨¼

èª•ç”ŸèŠ±è¨˜äº‹ç”Ÿæˆã®å“è³ªãƒ»ä¸€è²«æ€§ãƒ»SEOæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
"""

import pytest
import asyncio
import statistics
import time
from typing import Dict, List, Any, Optional, Tuple
from unittest.mock import AsyncMock, MagicMock, patch
from dataclasses import dataclass
import logging
import re

# Testå¯¾è±¡ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.content.deep_research_content_generator import DeepResearchContentGenerator
from src.seo.keyword_analyzer import KeywordAnalyzer
from src.services.ai.ai_service_manager import AIServiceManager

logger = logging.getLogger(__name__)


@dataclass
class ContentQualityMetrics:
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    overall_score: float
    readability_score: float
    seo_optimization_score: float
    factual_accuracy_score: float
    content_uniqueness_score: float
    keyword_density: Dict[str, float]
    content_length: int
    structure_score: float


@dataclass
class SEOAnalysisResult:
    """SEOåˆ†æçµæœ"""
    title_optimization: float
    meta_description_score: float
    heading_structure_score: float
    keyword_distribution_score: float
    internal_link_opportunities: int
    content_depth_score: float


class TestContentGenerationEngineQuality:
    """
    Content Generation Engine å“è³ªãƒ†ã‚¹ãƒˆ
    
    ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤: 9/10 (ã‚³ã‚¢æ©Ÿèƒ½)
    æŠ€è¡“è¤‡é›‘åº¦: 8/10 (é«˜)
    å¤–éƒ¨ä¾å­˜åº¦: 9/10 (æ¥µé«˜)
    ãƒ†ã‚¹ãƒˆå„ªå…ˆåº¦: CRITICAL
    """
    
    @pytest.fixture
    async def content_generator(self):
        """Content Generator ã®ãƒ†ã‚¹ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
        generator = DeepResearchContentGenerator()
        
        # ä¾å­˜é–¢ä¿‚ã®ãƒ¢ãƒƒã‚¯è¨­å®š
        generator.ai_service_manager = AsyncMock(spec=AIServiceManager)
        generator.keyword_analyzer = AsyncMock(spec=KeywordAnalyzer)
        
        # AI ã‚µãƒ¼ãƒ“ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ãƒ¢ãƒƒã‚¯è¨­å®š
        generator.ai_service_manager.generate_content_with_fallback.return_value = AsyncMock(
            success=True,
            content="Mock generated content about birth flowers...",
            provider="gemini",
            quality_score=85.0
        )
        
        return generator
    
    @pytest.fixture
    def birth_flower_test_data(self):
        """èª•ç”ŸèŠ±ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿"""
        return {
            "january": {
                "keywords": ["1æœˆ", "èª•ç”ŸèŠ±", "ã‚«ãƒ¼ãƒãƒ¼ã‚·ãƒ§ãƒ³", "èŠ±è¨€è‘‰", "ã‚¹ãƒãƒ¼ãƒ‰ãƒ­ãƒƒãƒ—"],
                "expected_elements": ["ã‚«ãƒ¼ãƒãƒ¼ã‚·ãƒ§ãƒ³", "ã‚¹ãƒãƒ¼ãƒ‰ãƒ­ãƒƒãƒ—", "èŠ±è¨€è‘‰", "è‚²ã¦æ–¹", "ç”±æ¥"],
                "seo_targets": {
                    "primary": "1æœˆ èª•ç”ŸèŠ±",
                    "secondary": ["ã‚«ãƒ¼ãƒãƒ¼ã‚·ãƒ§ãƒ³ èŠ±è¨€è‘‰", "èª•ç”ŸèŠ± 1æœˆç”Ÿã¾ã‚Œ", "1æœˆ èŠ±"]
                }
            },
            "march": {
                "keywords": ["3æœˆ", "èª•ç”ŸèŠ±", "æ¡œ", "ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—", "èŠ±è¨€è‘‰"],
                "expected_elements": ["æ¡œ", "ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ—", "èŠ±è¨€è‘‰", "é–‹èŠ±æ™‚æœŸ", "å“ç¨®"],
                "seo_targets": {
                    "primary": "3æœˆ èª•ç”ŸèŠ±",
                    "secondary": ["æ¡œ èŠ±è¨€è‘‰", "ãƒãƒ¥ãƒ¼ãƒªãƒƒãƒ— ç¨®é¡", "3æœˆ èŠ±"]
                }
            }
        }
    
    @pytest.mark.critical
    @pytest.mark.asyncio
    async def test_birth_flower_content_quality_consistency(self, content_generator, birth_flower_test_data):
        """
        èª•ç”ŸèŠ±ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ
        
        ãƒ“ã‚¸ãƒã‚¹ã‚·ãƒŠãƒªã‚ª: 12ãƒ¶æœˆåˆ†ã®èª•ç”ŸèŠ±è¨˜äº‹å“è³ªã®ä¸€è²«æ€§ç¢ºä¿
        æˆåŠŸåŸºæº–:
        - å“è³ªã‚¹ã‚³ã‚¢æ¨™æº–åå·®: 5ç‚¹ä»¥ä¸‹
        - æœ€ä½å“è³ªã‚¹ã‚³ã‚¢: 80ç‚¹ä»¥ä¸Š
        - ä¸€è²«æ€§ç‡: 95%ä»¥ä¸Š
        """
        
        logger.info("ğŸŒ¸ Testing birth flower content quality consistency...")
        
        quality_metrics = []
        generation_times = []
        
        # è¤‡æ•°æœˆã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å“è³ªæ¤œè¨¼
        for month, test_data in birth_flower_test_data.items():
            logger.info(f"ğŸ“ Generating content for {month}...")
            
            start_time = time.time()
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆå®Ÿè¡Œ
            result = await content_generator.generate_birth_flower_article(
                month=month,
                target_keywords=test_data["keywords"],
                seo_targets=test_data["seo_targets"],
                quality_mode="high"
            )
            
            generation_time = time.time() - start_time
            generation_times.append(generation_time)
            
            # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡
            metrics = await self._evaluate_content_quality(
                content=result.content,
                expected_elements=test_data["expected_elements"],
                target_keywords=test_data["keywords"]
            )
            
            quality_metrics.append(metrics)
            
            logger.info(f"   Quality Score: {metrics.overall_score:.1f}")
            logger.info(f"   Generation Time: {generation_time:.2f}s")
        
        # å“è³ªä¸€è²«æ€§åˆ†æ
        quality_scores = [m.overall_score for m in quality_metrics]
        
        if quality_scores:
            quality_std = statistics.stdev(quality_scores)
            min_quality = min(quality_scores)
            avg_quality = statistics.mean(quality_scores)
            
            # æˆåŠŸåŸºæº–æ¤œè¨¼
            assert quality_std <= 5.0, f"Quality std deviation {quality_std:.2f} exceeds 5.0 limit"
            assert min_quality >= 80.0, f"Minimum quality score {min_quality:.1f} below 80.0 threshold"
            
            # ä¸€è²«æ€§ç‡è¨ˆç®—
            quality_pairs = [(quality_scores[i], quality_scores[i+1]) 
                            for i in range(len(quality_scores)-1)]
            consistent_pairs = [pair for pair in quality_pairs 
                              if abs(pair[0] - pair[1]) <= 8.0]
            consistency_rate = len(consistent_pairs) / len(quality_pairs) if quality_pairs else 1.0
            
            assert consistency_rate >= 0.95, f"Consistency rate {consistency_rate:.2%} below 95% threshold"
            
            logger.info(f"ğŸ‰ Content quality consistency verified:")
            logger.info(f"   Average Quality: {avg_quality:.1f}")
            logger.info(f"   Quality Std Dev: {quality_std:.2f}")
            logger.info(f"   Minimum Quality: {min_quality:.1f}")
            logger.info(f"   Consistency Rate: {consistency_rate:.2%}")
    
    async def _evaluate_content_quality(self, content: str, expected_elements: List[str], 
                                      target_keywords: List[str]) -> ContentQualityMetrics:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡"""
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        content_length = len(content)
        word_count = len(content.split())
        
        # å¯èª­æ€§ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        readability_score = await self._calculate_readability_score(content)
        
        # SEOæœ€é©åŒ–ã‚¹ã‚³ã‚¢
        seo_score = await self._calculate_seo_optimization_score(content, target_keywords)
        
        # äº‹å®Ÿæ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢ï¼ˆè¦ç´ ã®å­˜åœ¨ç¢ºèªï¼‰
        factual_accuracy = self._calculate_factual_accuracy(content, expected_elements)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        uniqueness_score = await self._calculate_content_uniqueness(content)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦
        keyword_density = self._calculate_keyword_density(content, target_keywords)
        
        # æ§‹é€ ã‚¹ã‚³ã‚¢
        structure_score = self._calculate_structure_score(content)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = (
            readability_score * 0.25 +
            seo_score * 0.25 +
            factual_accuracy * 0.20 +
            uniqueness_score * 0.15 +
            structure_score * 0.15
        )
        
        return ContentQualityMetrics(
            overall_score=overall_score,
            readability_score=readability_score,
            seo_optimization_score=seo_score,
            factual_accuracy_score=factual_accuracy,
            content_uniqueness_score=uniqueness_score,
            keyword_density=keyword_density,
            content_length=content_length,
            structure_score=structure_score
        )
    
    async def _calculate_readability_score(self, content: str) -> float:
        """å¯èª­æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        sentences = content.split('ã€‚')
        words = content.split()
        
        if not sentences or not words:
            return 0.0
        
        avg_sentence_length = len(words) / len(sentences)
        
        # æ—¥æœ¬èªå¯èª­æ€§ã®ç°¡æ˜“è¨ˆç®—
        # é©åˆ‡ãªæ–‡é•·: 15-25èª
        if 15 <= avg_sentence_length <= 25:
            readability = 90.0
        elif 10 <= avg_sentence_length < 15:
            readability = 80.0
        elif 25 < avg_sentence_length <= 35:
            readability = 75.0
        else:
            readability = 60.0
        
        return readability
    
    async def _calculate_seo_optimization_score(self, content: str, keywords: List[str]) -> float:
        """SEOæœ€é©åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        content_lower = content.lower()
        total_words = len(content.split())
        
        seo_scores = []
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            keyword_count = content_lower.count(keyword_lower)
            keyword_density = keyword_count / total_words if total_words > 0 else 0
            
            # é©åˆ‡ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦: 1-3%
            if 0.01 <= keyword_density <= 0.03:
                seo_scores.append(90.0)
            elif 0.005 <= keyword_density < 0.01:
                seo_scores.append(75.0)
            elif 0.03 < keyword_density <= 0.05:
                seo_scores.append(70.0)
            else:
                seo_scores.append(50.0)
        
        return statistics.mean(seo_scores) if seo_scores else 0.0
    
    def _calculate_factual_accuracy(self, content: str, expected_elements: List[str]) -> float:
        """äº‹å®Ÿæ­£ç¢ºæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        content_lower = content.lower()
        found_elements = []
        
        for element in expected_elements:
            if element.lower() in content_lower:
                found_elements.append(element)
        
        accuracy_rate = len(found_elements) / len(expected_elements) if expected_elements else 1.0
        return accuracy_rate * 100.0
    
    async def _calculate_content_uniqueness(self, content: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        # ç°¡æ˜“å®Ÿè£…: ä¸€èˆ¬çš„ãªè¡¨ç¾ã®æ¤œå‡º
        common_phrases = [
            "ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™",
            "ã¨ã„ã†ã“ã¨ã§ã™",
            "ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™",
            "ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™"
        ]
        
        content_lower = content.lower()
        common_count = sum(1 for phrase in common_phrases if phrase in content_lower)
        
        # ä¸€èˆ¬çš„è¡¨ç¾ãŒå°‘ãªã„ã»ã©ç‹¬è‡ªæ€§ãŒé«˜ã„
        uniqueness = max(0, 100 - (common_count * 5))
        return uniqueness
    
    def _calculate_keyword_density(self, content: str, keywords: List[str]) -> Dict[str, float]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦è¨ˆç®—"""
        
        content_lower = content.lower()
        total_words = len(content.split())
        density_map = {}
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            keyword_count = content_lower.count(keyword_lower)
            density = (keyword_count / total_words * 100) if total_words > 0 else 0.0
            density_map[keyword] = round(density, 2)
        
        return density_map
    
    def _calculate_structure_score(self, content: str) -> float:
        """æ§‹é€ ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        # è¦‹å‡ºã—æ§‹é€ ã®è©•ä¾¡
        h2_count = content.count('##')
        h3_count = content.count('###')
        
        # æ®µè½æ•°
        paragraph_count = content.count('\n\n')
        
        # é©åˆ‡ãªæ§‹é€ ã®è©•ä¾¡
        structure_score = 60.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        if h2_count >= 3:  # é©åˆ‡ãªè¦‹å‡ºã—æ•°
            structure_score += 15.0
        
        if h3_count >= 2:  # è©³ç´°è¦‹å‡ºã—
            structure_score += 10.0
        
        if paragraph_count >= 5:  # é©åˆ‡ãªæ®µè½åˆ†ã‘
            structure_score += 15.0
        
        return min(100.0, structure_score)
    
    @pytest.mark.critical
    @pytest.mark.asyncio
    async def test_seo_optimization_effectiveness(self, content_generator, birth_flower_test_data):
        """
        SEOæœ€é©åŒ–åŠ¹æœãƒ†ã‚¹ãƒˆ
        
        ãƒ“ã‚¸ãƒã‚¹ã‚·ãƒŠãƒªã‚ª: æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ã®åŠ¹æœæ¸¬å®š
        æˆåŠŸåŸºæº–:
        - SEOæœ€é©åŒ–ã‚¹ã‚³ã‚¢: 85ç‚¹ä»¥ä¸Š
        - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦: 1-3%ã®ç¯„å›²
        - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å“è³ª: 90ç‚¹ä»¥ä¸Š
        """
        
        logger.info("ğŸ” Testing SEO optimization effectiveness...")
        
        seo_results = []
        
        for month, test_data in birth_flower_test_data.items():
            # SEOæœ€é©åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
            result = await content_generator.generate_seo_optimized_content(
                topic=f"{month} birth flowers",
                primary_keyword=test_data["seo_targets"]["primary"],
                secondary_keywords=test_data["seo_targets"]["secondary"],
                content_type="informational_article"
            )
            
            # SEOåˆ†æå®Ÿè¡Œ
            seo_analysis = await self._analyze_seo_optimization(
                content=result.content,
                title=result.title,
                meta_description=result.meta_description,
                target_keywords=test_data["keywords"]
            )
            
            seo_results.append(seo_analysis)
            
            logger.info(f"   {month.title()} SEO Score: {seo_analysis.title_optimization:.1f}")
        
        # SEOåŠ¹æœã®æ¤œè¨¼
        avg_title_optimization = statistics.mean([r.title_optimization for r in seo_results])
        avg_meta_score = statistics.mean([r.meta_description_score for r in seo_results])
        avg_keyword_distribution = statistics.mean([r.keyword_distribution_score for r in seo_results])
        
        # æˆåŠŸåŸºæº–æ¤œè¨¼
        assert avg_title_optimization >= 85.0, f"Title optimization {avg_title_optimization:.1f} below 85.0"
        assert avg_meta_score >= 90.0, f"Meta description score {avg_meta_score:.1f} below 90.0"
        assert avg_keyword_distribution >= 80.0, f"Keyword distribution {avg_keyword_distribution:.1f} below 80.0"
        
        logger.info(f"ğŸ‰ SEO optimization effectiveness verified:")
        logger.info(f"   Title Optimization: {avg_title_optimization:.1f}")
        logger.info(f"   Meta Description: {avg_meta_score:.1f}")
        logger.info(f"   Keyword Distribution: {avg_keyword_distribution:.1f}")
    
    async def _analyze_seo_optimization(self, content: str, title: str, 
                                      meta_description: str, target_keywords: List[str]) -> SEOAnalysisResult:
        """SEOæœ€é©åŒ–åˆ†æ"""
        
        # ã‚¿ã‚¤ãƒˆãƒ«æœ€é©åŒ–åˆ†æ
        title_score = self._analyze_title_optimization(title, target_keywords)
        
        # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³åˆ†æ
        meta_score = self._analyze_meta_description(meta_description, target_keywords)
        
        # è¦‹å‡ºã—æ§‹é€ åˆ†æ
        heading_score = self._analyze_heading_structure(content)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å¸ƒåˆ†æ
        keyword_score = self._analyze_keyword_distribution(content, target_keywords)
        
        # å†…éƒ¨ãƒªãƒ³ã‚¯æ©Ÿä¼š
        internal_links = self._count_internal_link_opportunities(content)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ·±åº¦åˆ†æ
        depth_score = self._analyze_content_depth(content)
        
        return SEOAnalysisResult(
            title_optimization=title_score,
            meta_description_score=meta_score,
            heading_structure_score=heading_score,
            keyword_distribution_score=keyword_score,
            internal_link_opportunities=internal_links,
            content_depth_score=depth_score
        )
    
    def _analyze_title_optimization(self, title: str, keywords: List[str]) -> float:
        """ã‚¿ã‚¤ãƒˆãƒ«æœ€é©åŒ–åˆ†æ"""
        
        score = 60.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        title_lower = title.lower()
        
        # ä¸»è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨
        for keyword in keywords[:2]:  # ä¸»è¦ãª2ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if keyword.lower() in title_lower:
                score += 20.0
        
        # ã‚¿ã‚¤ãƒˆãƒ«é•·ã®é©åˆ‡æ€§ï¼ˆ30-60æ–‡å­—ï¼‰
        if 30 <= len(title) <= 60:
            score += 10.0
        
        # æ•°å­—ã‚„å¹´å·ã®å­˜åœ¨ï¼ˆã‚¯ãƒªãƒƒã‚¯ç‡å‘ä¸Šï¼‰
        if re.search(r'\d{4}|\d+', title):
            score += 10.0
        
        return min(100.0, score)
    
    def _analyze_meta_description(self, meta_description: str, keywords: List[str]) -> float:
        """ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³åˆ†æ"""
        
        if not meta_description:
            return 0.0
        
        score = 70.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        meta_lower = meta_description.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å­˜åœ¨
        for keyword in keywords[:3]:  # ä¸»è¦ãª3ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            if keyword.lower() in meta_lower:
                score += 10.0
        
        # é©åˆ‡ãªé•·ã•ï¼ˆ120-160æ–‡å­—ï¼‰
        if 120 <= len(meta_description) <= 160:
            score += 10.0
        
        return min(100.0, score)
    
    def _analyze_heading_structure(self, content: str) -> float:
        """è¦‹å‡ºã—æ§‹é€ åˆ†æ"""
        
        h1_count = content.count('# ')
        h2_count = content.count('## ')
        h3_count = content.count('### ')
        
        score = 50.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # H1ã®å­˜åœ¨ï¼ˆ1ã¤ã®ã¿ãŒç†æƒ³ï¼‰
        if h1_count == 1:
            score += 20.0
        
        # H2ã®é©åˆ‡ãªæ•°ï¼ˆ3-6å€‹ãŒç†æƒ³ï¼‰
        if 3 <= h2_count <= 6:
            score += 20.0
        
        # H3ã®å­˜åœ¨ï¼ˆè©³ç´°æ§‹é€ ï¼‰
        if h3_count >= 2:
            score += 10.0
        
        return score
    
    def _analyze_keyword_distribution(self, content: str, keywords: List[str]) -> float:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å¸ƒåˆ†æ"""
        
        content_lower = content.lower()
        total_words = len(content.split())
        
        distribution_scores = []
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            positions = [i for i, word in enumerate(content_lower.split()) if keyword_lower in word]
            
            if positions:
                # å‡ç­‰åˆ†å¸ƒã®è©•ä¾¡
                distribution_variance = statistics.variance(positions) if len(positions) > 1 else 0
                max_variance = (total_words ** 2) / 4  # ç†è«–çš„æœ€å¤§åˆ†æ•£
                
                distribution_score = max(0, 100 - (distribution_variance / max_variance * 100))
                distribution_scores.append(distribution_score)
        
        return statistics.mean(distribution_scores) if distribution_scores else 0.0
    
    def _count_internal_link_opportunities(self, content: str) -> int:
        """å†…éƒ¨ãƒªãƒ³ã‚¯æ©Ÿä¼šã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        
        # é–¢é€£ãƒˆãƒ”ãƒƒã‚¯ã®æ¤œå‡ºï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        link_opportunities = [
            "èª•ç”ŸèŠ±",
            "èŠ±è¨€è‘‰", 
            "è‚²ã¦æ–¹",
            "èŠ±ã®ç¨®é¡",
            "ã‚¬ãƒ¼ãƒ‡ãƒ‹ãƒ³ã‚°"
        ]
        
        content_lower = content.lower()
        opportunities = sum(1 for term in link_opportunities if term in content_lower)
        
        return opportunities
    
    def _analyze_content_depth(self, content: str) -> float:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ·±åº¦åˆ†æ"""
        
        word_count = len(content.split())
        
        # æ–‡ç« ã®è©³ç´°åº¦è©•ä¾¡
        depth_indicators = [
            "ã«ã¤ã„ã¦è©³ã—ã",
            "å…·ä½“çš„ã«ã¯",
            "ä¾‹ãˆã°",
            "ä¸€æ–¹ã§",
            "ã•ã‚‰ã«",
            "ã¾ãŸ",
            "ãªãœãªã‚‰"
        ]
        
        content_lower = content.lower()
        depth_count = sum(1 for indicator in depth_indicators if indicator in content_lower)
        
        # æ·±åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        base_score = min(80, word_count / 50)  # èªæ•°ã«ã‚ˆã‚‹åŸºæœ¬ã‚¹ã‚³ã‚¢
        depth_bonus = min(20, depth_count * 3)  # æ·±åº¦æŒ‡æ¨™ãƒœãƒ¼ãƒŠã‚¹
        
        return base_score + depth_bonus
    
    @pytest.mark.critical
    @pytest.mark.asyncio
    async def test_content_generation_performance_under_load(self, content_generator):
        """
        è² è·æ™‚ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ
        
        ãƒ“ã‚¸ãƒã‚¹ã‚·ãƒŠãƒªã‚ª: é«˜è² è·æ™‚ã®å®‰å®šã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
        æˆåŠŸåŸºæº–:
        - åŒæ™‚ç”Ÿæˆæ•°: 20è¨˜äº‹
        - å¹³å‡ç”Ÿæˆæ™‚é–“: 45ç§’ä»¥å†…
        - æˆåŠŸç‡: 90%ä»¥ä¸Š
        """
        
        logger.info("âš¡ Testing content generation performance under load...")
        
        # ä¸¦è¡Œã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚¿ã‚¹ã‚¯
        generation_tasks = []
        
        for i in range(20):
            month = (i % 12) + 1
            task = content_generator.generate_birth_flower_article(
                month=f"month_{month}",
                target_keywords=[f"{month}æœˆ", "èª•ç”ŸèŠ±", "èŠ±è¨€è‘‰"],
                quality_mode="balanced",
                request_id=f"load_test_{i}"
            )
            generation_tasks.append(task)
        
        # ä¸¦è¡Œå®Ÿè¡Œ
        start_time = time.time()
        results = await asyncio.gather(*generation_tasks, return_exceptions=True)
        total_time = time.time() - start_time
        
        # çµæœåˆ†æ
        successful_results = [r for r in results if not isinstance(r, Exception)]
        failed_results = [r for r in results if isinstance(r, Exception)]
        
        success_rate = len(successful_results) / len(results)
        avg_quality = statistics.mean([r.quality_score for r in successful_results]) if successful_results else 0
        
        # æˆåŠŸåŸºæº–æ¤œè¨¼
        assert success_rate >= 0.90, f"Success rate {success_rate:.2%} below 90% threshold"
        assert total_time <= 45.0, f"Total generation time {total_time:.2f}s exceeds 45s limit"
        
        logger.info(f"ğŸ‰ Load test completed:")
        logger.info(f"   Success Rate: {success_rate:.2%}")
        logger.info(f"   Average Quality: {avg_quality:.1f}")
        logger.info(f"   Total Time: {total_time:.2f}s")
        logger.info(f"   Failed: {len(failed_results)}")


if __name__ == "__main__":
    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ
    pytest.main([
        __file__,
        "-v", 
        "-m", "critical",
        "--tb=short"
    ])